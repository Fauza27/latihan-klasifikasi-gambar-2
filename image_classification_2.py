# -*- coding: utf-8 -*-
"""Image Classification 2.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1ZBfvwIQnqBfPwZJ6VJowa8Vu8sk_BeVX
"""

import tensorflow as tf
from tensorflow.keras import layers, models
from tensorflow.keras.preprocessing.image import ImageDataGenerator
from tensorflow.keras.applications import MobileNetV2
from tensorflow.keras.callbacks import Callback, ReduceLROnPlateau
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
import os
import shutil
import zipfile

! chmod 600 /content/kaggle.json

!KAGGLE_CONFIG_DIR=/content/ kaggle datasets download -d utkarshsaxenadn/fast-food-classification-dataset

zip_file = zipfile.ZipFile('/content/fast-food-classification-dataset.zip', 'r')
zip_file.extractall('/tmp/')

original_dataset_dir = '/tmp/Fast Food Classification V2/Train'

# Path to create train and test directories
base_dir = 'path/to/base_dir'
os.makedirs(base_dir, exist_ok=True)

# Creating train and test directories
train_dir = os.path.join(base_dir, 'train')
os.makedirs(train_dir, exist_ok=True)
test_dir = os.path.join(base_dir, 'test')
os.makedirs(test_dir, exist_ok=True)

# List of class directories in the original dataset
class_directories = os.listdir(original_dataset_dir)

for class_dir in class_directories:
    class_path = os.path.join(original_dataset_dir, class_dir)

    # Split the images into 80% train and 20% test
    train_class_path, test_class_path = train_test_split(os.listdir(class_path), test_size=0.2, random_state=42)

    # Create train class directory
    train_class_dir = os.path.join(train_dir, class_dir)
    os.makedirs(train_class_dir, exist_ok=True)

    # Create test class directory
    test_class_dir = os.path.join(test_dir, class_dir)
    os.makedirs(test_class_dir, exist_ok=True)

    # Copy images to train class directory
    for img in train_class_path:
        shutil.copy(os.path.join(class_path, img), os.path.join(train_class_dir, img))

    # Copy images to test class directory
    for img in test_class_path:
        shutil.copy(os.path.join(class_path, img), os.path.join(test_class_dir, img))

img_width, img_height = 224, 224
input_shape = (img_width, img_height, 3)

base_model = MobileNetV2(input_shape=input_shape, include_top=False, weights='imagenet')

# Freeze the layers of the pre-trained model
for layer in base_model.layers:
    layer.trainable = False

train_datagen = ImageDataGenerator(rescale=1./255,
                                   shear_range=0.2,
                                   zoom_range=0.2,
                                   rotation_range=30,
                                   width_shift_range=0.2,
                                   height_shift_range=0.2,
                                   fill_mode='nearest',
                                   horizontal_flip=True)

# Rescaling for the test set
test_datagen = ImageDataGenerator(rescale=1./255)

# Creating data generators
train_generator = train_datagen.flow_from_directory(train_dir,
                                                    target_size=(img_width, img_height),
                                                    batch_size=32,
                                                    class_mode='categorical')

test_generator = test_datagen.flow_from_directory(test_dir,
                                                  target_size=(img_width, img_height),
                                                  batch_size=32,
                                                  class_mode='categorical')

model = models.Sequential()

# Add the pre-trained base model
model.add(base_model)

# Add a Conv2D layer
model.add(layers.Conv2D(64, (3, 3), activation='relu', padding='same'))
model.add(layers.MaxPooling2D((2, 2)))

# Add a Conv2D layer
model.add(layers.Conv2D(128, (3, 3), activation='relu', padding='same'))
model.add(layers.MaxPooling2D((2, 2)))

# Add a Conv2D layer
model.add(layers.Conv2D(256, (3, 3), activation='relu', padding='same'))

# Add GlobalAveragePooling2D layer
model.add(layers.GlobalAveragePooling2D())

# Add a Dense layer
model.add(layers.Dense(512, activation='relu'))

# Add the output layer
model.add(layers.Dense(10, activation='softmax'))  # Assuming 10 classes

model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])

class CustomCallback(Callback):
    def on_epoch_end(self, epoch, logs=None):
        if logs.get('accuracy') > 0.92 and logs.get('val_accuracy') > 0.92:
            print("\nTraining stopped as accuracy is above 92% on both training and validation sets.")
            self.model.stop_training = True

reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=5, min_lr=1e-7, verbose=1)

history = model.fit_generator(train_generator,
                              steps_per_epoch=train_generator.samples // train_generator.batch_size,
                              epochs=50,  # You can adjust the number of epochs
                              validation_data=test_generator,
                              validation_steps=test_generator.samples // test_generator.batch_size,
                              callbacks=[CustomCallback(), reduce_lr])

plt.plot(history.history['accuracy'], label='Training Accuracy')
plt.plot(history.history['val_accuracy'], label='Validation Accuracy')
plt.xlabel('Epochs')
plt.ylabel('Accuracy')
plt.legend()
plt.show()

plt.plot(history.history['loss'], label='Training Loss')
plt.plot(history.history['val_loss'], label='Validation Loss')
plt.xlabel('Epochs')
plt.ylabel('Loss')
plt.legend()
plt.show()

model.save('transfer_learning_model.h5')
converter = tf.lite.TFLiteConverter.from_keras_model(model)
tflite_model = converter.convert()
with open('transfer_learning_model.tflite', 'wb') as f:
    f.write(tflite_model)